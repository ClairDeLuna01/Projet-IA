{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Union, Dict, Generic, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>our rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>Distracted driving causes more deaths in Canad...</td>\n",
       "      <td>You Can Be Fined $1,500 If Your Passenger Is U...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>Missouri politicians have made statements afte...</td>\n",
       "      <td>Missouri lawmakers condemn Las Vegas shooting</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>Home Alone 2: Lost in New York is full of viol...</td>\n",
       "      <td>CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>But things took a turn for the worse when riot...</td>\n",
       "      <td>Obama’s Daughters Caught on Camera Burning US ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>It’s no secret that Epstein and Schiff share a...</td>\n",
       "      <td>Leaked Visitor Logs Reveal Schiff’s 78 Visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>47423bb6</td>\n",
       "      <td>More than four million calls to the taxman are...</td>\n",
       "      <td>Taxman fails to answer four million calls a ye...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>097c142a</td>\n",
       "      <td>More under-18s are being taken to court for se...</td>\n",
       "      <td>Police catch 11‑year‑olds being used to sell d...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>08bc59f4</td>\n",
       "      <td>The Government’s much vaunted Help to Buy Isa ...</td>\n",
       "      <td>Help to Buy Isa scandal: 500,000 first-time bu...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>af3393ce</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>a39d07df</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     public_id                                               text  \\\n",
       "0     5a228e0e  Distracted driving causes more deaths in Canad...   \n",
       "1     30c605a1  Missouri politicians have made statements afte...   \n",
       "2     c3dea290  Home Alone 2: Lost in New York is full of viol...   \n",
       "3     f14e8eb6  But things took a turn for the worse when riot...   \n",
       "4     faf024d6  It’s no secret that Epstein and Schiff share a...   \n",
       "...        ...                                                ...   \n",
       "1259  47423bb6  More than four million calls to the taxman are...   \n",
       "1260  097c142a  More under-18s are being taken to court for se...   \n",
       "1261  08bc59f4  The Government’s much vaunted Help to Buy Isa ...   \n",
       "1262  af3393ce  The late Robin Williams once called cocaine “G...   \n",
       "1263  a39d07df  The late Robin Williams once called cocaine “G...   \n",
       "\n",
       "                                                  title our rating  \n",
       "0     You Can Be Fined $1,500 If Your Passenger Is U...      false  \n",
       "1         Missouri lawmakers condemn Las Vegas shooting    mixture  \n",
       "2     CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...    mixture  \n",
       "3     Obama’s Daughters Caught on Camera Burning US ...      false  \n",
       "4     Leaked Visitor Logs Reveal Schiff’s 78 Visits ...      false  \n",
       "...                                                 ...        ...  \n",
       "1259  Taxman fails to answer four million calls a ye...       true  \n",
       "1260  Police catch 11‑year‑olds being used to sell d...       true  \n",
       "1261  Help to Buy Isa scandal: 500,000 first-time bu...      false  \n",
       "1262           A coke-snorting generation of hypocrites       true  \n",
       "1263           A coke-snorting generation of hypocrites       true  \n",
       "\n",
       "[1264 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain = pd.read_csv('data/HAI817_Projet_train.csv')\n",
    "dataTest = pd.read_csv('data/HAI817_Projet_test.csv')\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk download\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distracted driving causes deaths canada impaired driving every province territory laws driving operating cell phone tell passengers stay phones driving measures necessary distracted driving claimed lives impaired driving provinces like british columbia ontario quebec alberta nova scotia manitoba newfoundland labrador mobile phones even held passenger dangerous distraction driver starting next week distracted screen held passenger attracts penalty three demerit points drivers screens mix matter holding device using facetime taking selfies driver showing driver funny cat video provinces mobile phone categorised visual display unit meaning considered akin television screen important practice safe driving sake fellow drivers canada cracking distracted driving problem rollout stricter laws impose harsher penalties heftier fines guilty offenders taking effect next week adds serious penalties convicted distracted driving'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # remove punctuation\n",
    "    words = [word for word in words if word not in punctuations]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "XpreprocessTrain = dataTrain['text'].apply(remove_stop_words)\n",
    "XpreprocessTest = dataTest['text'].apply(remove_stop_words)\n",
    "ytxtTrain = dataTrain['our rating']\n",
    "ytxtTest = dataTest['our rating']\n",
    "XpreprocessTrain[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  1,  6,  7,  8,  9,  1, 10, 11, 12, 13, 14,\n",
       "        15, 16,  1, 17, 18,  0,  1, 19, 20,  5,  1, 21, 22, 23, 24, 25, 26,\n",
       "        27, 28, 29, 30, 31, 32, 33, 16, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
       "         0, 43, 35, 36, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
       "        57, 58, 39, 59, 39, 60, 61, 62, 21, 33, 12, 63, 64, 65, 66, 67, 68,\n",
       "        69, 70, 43, 71, 72, 73,  1, 74, 75, 49,  4, 76,  0,  1, 77, 78, 79,\n",
       "         9, 80, 81, 82, 83, 84, 85, 86, 57, 87, 41, 42, 88, 89, 82, 90,  0,\n",
       "         1], dtype=int64),\n",
       " array([0], dtype=uint8))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "class Tokenizer:\n",
    "    def __init__(self, dtype: type = np.int64):\n",
    "        self.txt2token = {}\n",
    "        self.token2txt = {}\n",
    "        self.tokens = set()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, text: np.ndarray):\n",
    "        count = 0\n",
    "        for txt in text:\n",
    "            for word in txt.split():\n",
    "                if word not in self.txt2token:\n",
    "                    self.txt2token[word] = count\n",
    "                    self.token2txt[count] = word\n",
    "                    count += 1\n",
    "                self.tokens.add(self.txt2token[word])\n",
    "\n",
    "    def histogram(self, text):\n",
    "        tokenized_text = np.zeros((len(text), len(self.tokens)), dtype=self.dtype)\n",
    "        for i, txt in enumerate(text):\n",
    "            for word in txt.split():\n",
    "                tokenized_text[i][self.txt2token[word]] += 1\n",
    "\n",
    "        return tokenized_text\n",
    "    \n",
    "    def transform(self, text):\n",
    "        tokenized_text = []\n",
    "        for txt in text:\n",
    "            tokenized_text.append(np.array([self.txt2token[word] for word in txt.split() if word in self.txt2token], dtype=self.dtype))\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "Xtokenizer = Tokenizer(dtype=np.int64)\n",
    "Xtokenizer.fit(np.concatenate((np.array(XpreprocessTrain), np.array(XpreprocessTest))))\n",
    "XtokenTrain = Xtokenizer.transform(XpreprocessTrain)\n",
    "XhistTrain = Xtokenizer.histogram(XpreprocessTrain)\n",
    "XtokenTest = Xtokenizer.transform(XpreprocessTest)\n",
    "XhistTest = Xtokenizer.histogram(XpreprocessTest)\n",
    "\n",
    "Ytokenizer = Tokenizer(dtype=np.uint8)\n",
    "Ytokenizer.fit(np.concatenate((np.array(ytxtTrain), np.array(ytxtTest))))\n",
    "YtokenTrain = Ytokenizer.transform(ytxtTrain)\n",
    "YtokenTest = Ytokenizer.transform(ytxtTest)\n",
    "\n",
    "XtokenTrain[0], YtokenTrain[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54738562])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification\n",
    "# naive bayes\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_prior = {}\n",
    "        self.word_likelihood = {}\n",
    "        self.vocab = set()\n",
    "        self.word_count = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(np.concatenate(y))\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        # Initialize word count and class count\n",
    "        self.word_count = {c: Counter() for c in self.classes}\n",
    "        class_count = Counter()\n",
    "\n",
    "        for i in range(total_samples):\n",
    "            label = y[i][0]\n",
    "            class_count[label] += 1\n",
    "            for word in X[i]:\n",
    "                self.word_count[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "        # Compute class prior probabilities P(C)\n",
    "        self.class_prior = {c: count / total_samples for c, count in class_count.items()}\n",
    "        \n",
    "        # Compute word likelihood P(W|C) with Laplace smoothing\n",
    "        self.word_likelihood = {c: {} for c in self.classes}\n",
    "        vocab_size = len(self.vocab)\n",
    "        for c in self.classes:\n",
    "            total_words = sum(self.word_count[c].values())\n",
    "            for word in self.vocab:\n",
    "                # Apply Laplace smoothing\n",
    "                self.word_likelihood[c][word] = (self.word_count[c][word] + 1) / (total_words + vocab_size)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            log_probs = {}\n",
    "            for c in self.classes:\n",
    "                log_probs[c] = np.log(self.class_prior[c])\n",
    "                for word in sample:\n",
    "                    if word in self.vocab:  # Only consider words seen in training\n",
    "                        log_probs[c] += np.log(self.word_likelihood[c][word])\n",
    "                    else:\n",
    "                        # Apply Laplace smoothing for unseen words\n",
    "                        log_probs[c] += np.log(1 / (sum(self.word_count[c].values()) + len(self.vocab)))\n",
    "            \n",
    "            predicted_class = max(log_probs, key=log_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "# Train the model\n",
    "model = NaiveBayesClassifier()\n",
    "model.fit(XtokenTrain, YtokenTrain)\n",
    "\n",
    "# Predict the test set\n",
    "Ypred = model.predict(XtokenTest)\n",
    "\n",
    "accuracySum = 0\n",
    "for i in range(len(Ypred)):\n",
    "    accuracySum += Ypred[i] == YtokenTest[i]\n",
    "accuracy = accuracySum / len(Ypred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/40], Loss: 1.4017424583435059\n",
      "Epoch [1/10], Step [2/40], Loss: 1.391738772392273\n",
      "Epoch [1/10], Step [3/40], Loss: 1.3836076259613037\n",
      "Epoch [1/10], Step [4/40], Loss: 1.3734073638916016\n",
      "Epoch [1/10], Step [5/40], Loss: 1.3645902872085571\n",
      "Epoch [1/10], Step [6/40], Loss: 1.3603006601333618\n",
      "Epoch [1/10], Step [7/40], Loss: 1.3504338264465332\n",
      "Epoch [1/10], Step [8/40], Loss: 1.3403682708740234\n",
      "Epoch [1/10], Step [9/40], Loss: 1.3311219215393066\n",
      "Epoch [1/10], Step [10/40], Loss: 1.3099629878997803\n",
      "Epoch [1/10], Step [11/40], Loss: 1.294071078300476\n",
      "Epoch [1/10], Step [12/40], Loss: 1.2838865518569946\n",
      "Epoch [1/10], Step [13/40], Loss: 1.2620855569839478\n",
      "Epoch [1/10], Step [14/40], Loss: 1.2543872594833374\n",
      "Epoch [1/10], Step [15/40], Loss: 1.2485963106155396\n",
      "Epoch [1/10], Step [16/40], Loss: 1.1748766899108887\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m     YtokenTrain1D \u001b[38;5;241m=\u001b[39m YtokenTrain1D\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     96\u001b[0m     YtokenTest1D \u001b[38;5;241m=\u001b[39m YtokenTest1D\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 99\u001b[0m \u001b[43mtrain_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtokenTrain1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m    102\u001b[0m test_nn(Xtest, YtokenTest1D, model)\n",
      "Cell \u001b[1;32mIn[93], line 37\u001b[0m, in \u001b[0;36mtrain_nn\u001b[1;34m(X, y, model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (texts, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     38\u001b[0m         texts \u001b[38;5;241m=\u001b[39m texts\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     39\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classification\n",
    "# neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def train_nn(X, y, model, criterion, optimizer, num_epochs=5):\n",
    "    train_dataset = TextDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts = texts.float()\n",
    "            labels = labels.long()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "def test_nn(X: torch.Tensor, y: torch.Tensor, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for texts, labels in zip(X, y):\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 0)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy: {100 * correct / total}%')\n",
    "        \n",
    "# Hyperparameters\n",
    "input_size = len(Xtokenizer.tokens)\n",
    "hidden_size = 500\n",
    "num_classes = len(Ytokenizer.tokens)\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "YtokenTrain1D = torch.tensor([y[0] / num_classes for y in YtokenTrain]).float()\n",
    "YtokenTest1D = torch.tensor([y[0] / num_classes for y in YtokenTest]).float()\n",
    "maxval = np.max(np.concatenate((XhistTrain, XhistTest)))\n",
    "Xtrain = torch.tensor(XhistTrain / maxval).float()\n",
    "Xtest = torch.tensor(XhistTest / maxval).float()\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    Xtrain = Xtrain.to(device)\n",
    "    Xtest = Xtest.to(device)\n",
    "    YtokenTrain1D = YtokenTrain1D.to(device)\n",
    "    YtokenTest1D = YtokenTest1D.to(device)\n",
    "    \n",
    "\n",
    "train_nn(Xtrain, YtokenTrain1D, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Test the model\n",
    "test_nn(Xtest, YtokenTest1D, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
