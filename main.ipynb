{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Union, Dict, Generic, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>our rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>Distracted driving causes more deaths in Canad...</td>\n",
       "      <td>You Can Be Fined $1,500 If Your Passenger Is U...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>Missouri politicians have made statements afte...</td>\n",
       "      <td>Missouri lawmakers condemn Las Vegas shooting</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>Home Alone 2: Lost in New York is full of viol...</td>\n",
       "      <td>CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>But things took a turn for the worse when riot...</td>\n",
       "      <td>Obama’s Daughters Caught on Camera Burning US ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>It’s no secret that Epstein and Schiff share a...</td>\n",
       "      <td>Leaked Visitor Logs Reveal Schiff’s 78 Visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>47423bb6</td>\n",
       "      <td>More than four million calls to the taxman are...</td>\n",
       "      <td>Taxman fails to answer four million calls a ye...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>097c142a</td>\n",
       "      <td>More under-18s are being taken to court for se...</td>\n",
       "      <td>Police catch 11‑year‑olds being used to sell d...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>08bc59f4</td>\n",
       "      <td>The Government’s much vaunted Help to Buy Isa ...</td>\n",
       "      <td>Help to Buy Isa scandal: 500,000 first-time bu...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>af3393ce</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>a39d07df</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     public_id                                               text  \\\n",
       "0     5a228e0e  Distracted driving causes more deaths in Canad...   \n",
       "1     30c605a1  Missouri politicians have made statements afte...   \n",
       "2     c3dea290  Home Alone 2: Lost in New York is full of viol...   \n",
       "3     f14e8eb6  But things took a turn for the worse when riot...   \n",
       "4     faf024d6  It’s no secret that Epstein and Schiff share a...   \n",
       "...        ...                                                ...   \n",
       "1259  47423bb6  More than four million calls to the taxman are...   \n",
       "1260  097c142a  More under-18s are being taken to court for se...   \n",
       "1261  08bc59f4  The Government’s much vaunted Help to Buy Isa ...   \n",
       "1262  af3393ce  The late Robin Williams once called cocaine “G...   \n",
       "1263  a39d07df  The late Robin Williams once called cocaine “G...   \n",
       "\n",
       "                                                  title our rating  \n",
       "0     You Can Be Fined $1,500 If Your Passenger Is U...      false  \n",
       "1         Missouri lawmakers condemn Las Vegas shooting    mixture  \n",
       "2     CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...    mixture  \n",
       "3     Obama’s Daughters Caught on Camera Burning US ...      false  \n",
       "4     Leaked Visitor Logs Reveal Schiff’s 78 Visits ...      false  \n",
       "...                                                 ...        ...  \n",
       "1259  Taxman fails to answer four million calls a ye...       true  \n",
       "1260  Police catch 11‑year‑olds being used to sell d...       true  \n",
       "1261  Help to Buy Isa scandal: 500,000 first-time bu...      false  \n",
       "1262           A coke-snorting generation of hypocrites       true  \n",
       "1263           A coke-snorting generation of hypocrites       true  \n",
       "\n",
       "[1264 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain = pd.read_csv('data/HAI817_Projet_train.csv')\n",
    "dataTest = pd.read_csv('data/HAI817_Projet_test.csv')\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk download\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distracted driving causes deaths canada impaired driving every province territory laws driving operating cell phone tell passengers stay phones driving measures necessary distracted driving claimed lives impaired driving provinces like british columbia ontario quebec alberta nova scotia manitoba newfoundland labrador mobile phones even held passenger dangerous distraction driver starting next week distracted screen held passenger attracts penalty three demerit points drivers screens mix matter holding device using facetime taking selfies driver showing driver funny cat video provinces mobile phone categorised visual display unit meaning considered akin television screen important practice safe driving sake fellow drivers canada cracking distracted driving problem rollout stricter laws impose harsher penalties heftier fines guilty offenders taking effect next week adds serious penalties convicted distracted driving'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # remove punctuation\n",
    "    words = [word for word in words if word not in punctuations]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "XpreprocessTrain = dataTrain['text'].apply(remove_stop_words)\n",
    "XpreprocessTest = dataTest['text'].apply(remove_stop_words)\n",
    "ytxtTrain = dataTrain['our rating']\n",
    "ytxtTest = dataTest['our rating']\n",
    "XpreprocessTrain[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  1,  6,  7,  8,  9,  1, 10, 11, 12, 13, 14,\n",
       "        15, 16,  1, 17, 18,  0,  1, 19, 20,  5,  1, 21, 22, 23, 24, 25, 26,\n",
       "        27, 28, 29, 30, 31, 32, 33, 16, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
       "         0, 43, 35, 36, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
       "        57, 58, 39, 59, 39, 60, 61, 62, 21, 33, 12, 63, 64, 65, 66, 67, 68,\n",
       "        69, 70, 43, 71, 72, 73,  1, 74, 75, 49,  4, 76,  0,  1, 77, 78, 79,\n",
       "         9, 80, 81, 82, 83, 84, 85, 86, 57, 87, 41, 42, 88, 89, 82, 90,  0,\n",
       "         1], dtype=int64),\n",
       " array([0], dtype=uint8),\n",
       " dict_keys(['false', 'mixture', 'other', 'true']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "class Tokenizer:\n",
    "    def __init__(self, dtype: type = np.int64):\n",
    "        self.txt2token = {}\n",
    "        self.token2txt = {}\n",
    "        self.tokens = set()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, text: np.ndarray):\n",
    "        count = 0\n",
    "        for txt in text:\n",
    "            for word in txt.split():\n",
    "                if word not in self.txt2token:\n",
    "                    self.txt2token[word] = count\n",
    "                    self.token2txt[count] = word\n",
    "                    count += 1\n",
    "                self.tokens.add(self.txt2token[word])\n",
    "\n",
    "    def histogram(self, text):\n",
    "        tokenized_text = np.zeros((len(text), len(self.tokens)), dtype=self.dtype)\n",
    "        for i, txt in enumerate(text):\n",
    "            for word in txt.split():\n",
    "                tokenized_text[i][self.txt2token[word]] += 1\n",
    "\n",
    "        return tokenized_text\n",
    "    \n",
    "    def transform(self, text) -> list[np.ndarray]:\n",
    "        tokenized_text = []\n",
    "        for txt in text:\n",
    "            tokenized_text.append(np.array([self.txt2token[word] for word in txt.split() if word in self.txt2token], dtype=self.dtype))\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "Xtokenizer = Tokenizer(dtype=np.int64)\n",
    "Xtokenizer.fit(np.concatenate((np.array(XpreprocessTrain), np.array(XpreprocessTest))))\n",
    "XtokenTrain = Xtokenizer.transform(XpreprocessTrain)\n",
    "XhistTrain = Xtokenizer.histogram(XpreprocessTrain)\n",
    "XtokenTest = Xtokenizer.transform(XpreprocessTest)\n",
    "XhistTest = Xtokenizer.histogram(XpreprocessTest)\n",
    "\n",
    "Ytokenizer = Tokenizer(dtype=np.uint8)\n",
    "Ytokenizer.fit(np.concatenate((np.array(ytxtTrain), np.array(ytxtTest))))\n",
    "YtokenTrain = Ytokenizer.transform(ytxtTrain)\n",
    "YtokenTest = Ytokenizer.transform(ytxtTest)\n",
    "\n",
    "XtokenTrain[0], YtokenTrain[0], Ytokenizer.txt2token.keys()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_get_column_indices' from 'sklearn.utils' (c:\\Python39\\lib\\site-packages\\sklearn\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# oversampling and undersampling\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditedNearestNeighbours\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_adasyn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ADASYN\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_random_over_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC, SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADASYN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomOverSampler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, SMOTEN, SMOTENC\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeansSMOTE\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVMSMOTE, BorderlineSMOTE\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoder, OrdinalEncoder\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     _get_column_indices,\n\u001b[0;32m     20\u001b[0m     _safe_indexing,\n\u001b[0;32m     21\u001b[0m     check_array,\n\u001b[0;32m     22\u001b[0m     check_random_state,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsefuncs_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     csr_mean_variance_axis0,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_features\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_get_column_indices' from 'sklearn.utils' (c:\\Python39\\lib\\site-packages\\sklearn\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# oversampling and undersampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.54738562]\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "# naive bayes\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_prior = {}\n",
    "        self.word_likelihood = {}\n",
    "        self.vocab = set()\n",
    "        self.word_count = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(np.concatenate(y))\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        # Initialize word count and class count\n",
    "        self.word_count = {c: Counter() for c in self.classes}\n",
    "        class_count = Counter()\n",
    "\n",
    "        for i in range(total_samples):\n",
    "            label = y[i][0]\n",
    "            class_count[label] += 1\n",
    "            for word in X[i]:\n",
    "                self.word_count[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "        # Compute class prior probabilities P(C)\n",
    "        self.class_prior = {c: count / total_samples for c, count in class_count.items()}\n",
    "        \n",
    "        # Compute word likelihood P(W|C) with Laplace smoothing\n",
    "        self.word_likelihood = {c: {} for c in self.classes}\n",
    "        vocab_size = len(self.vocab)\n",
    "        for c in self.classes:\n",
    "            total_words = sum(self.word_count[c].values())\n",
    "            for word in self.vocab:\n",
    "                # Apply Laplace smoothing\n",
    "                self.word_likelihood[c][word] = (self.word_count[c][word] + 1) / (total_words + vocab_size)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            log_probs = {}\n",
    "            for c in self.classes:\n",
    "                log_probs[c] = np.log(self.class_prior[c])\n",
    "                for word in sample:\n",
    "                    if word in self.vocab:  # Only consider words seen in training\n",
    "                        log_probs[c] += np.log(self.word_likelihood[c][word])\n",
    "                    else:\n",
    "                        # Apply Laplace smoothing for unseen words\n",
    "                        log_probs[c] += np.log(1 / (sum(self.word_count[c].values()) + len(self.vocab)))\n",
    "            \n",
    "            predicted_class = max(log_probs, key=log_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "# Train the model\n",
    "model = NaiveBayesClassifier()\n",
    "model.fit(XtokenTrain, YtokenTrain)\n",
    "\n",
    "# Predict the test set\n",
    "Ypred = model.predict(XtokenTest)\n",
    "\n",
    "confusMatrix = np.zeros((len(Ytokenizer.tokens), len(Ytokenizer.tokens)), dtype=np.int64)\n",
    "\n",
    "accuracySum = 0\n",
    "for i in range(len(Ypred)):\n",
    "    accuracySum += Ypred[i] == YtokenTest[i]\n",
    "    confusMatrix[YtokenTest[i][0]][Ypred[i]] += 1\n",
    "accuracy = accuracySum / len(Ypred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "\t\tfalse (real)\tmixture (real)\tother (real)\ttrue (real)\n",
      "false (pred)\t279\t\t17\t\t9\t\t10\n",
      "mixture (pred)\t32\t\t18\t\t2\t\t4\n",
      "other (pred)\t27\t\t4\t\t0\t\t0\n",
      "true (pred)\t135\t\t36\t\t1\t\t38\n",
      "\n",
      "Recall (false) : 0.886\n",
      "Precision (false) : 0.59\n",
      "\n",
      "Recall (mixture) : 0.321\n",
      "Precision (mixture) : 0.24\n",
      "\n",
      "Recall (other) : 0.0\n",
      "Precision (other) : 0.0\n",
      "\n",
      "Recall (true) : 0.181\n",
      "Precision (true) : 0.731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printConfusionMatrix(confusionMatrix):\n",
    "    print('Confusion matrix:')\n",
    "    # format nicely with column and row headers\n",
    "    print('\\t' + ''.join([\"\\t\" + s + \" (real)\" for s in Ytokenizer.txt2token.keys()]))\n",
    "    for i in range(len(Ytokenizer.txt2token.keys())):\n",
    "        print(list(Ytokenizer.txt2token.keys())[i] + ' (pred)', end='\\t')\n",
    "        print('\\t\\t'.join(str(confusionMatrix[i][j]) for j in range(len(Ytokenizer.tokens))))\n",
    "        \n",
    "    print()\n",
    "\n",
    "    # compute recall and precision\n",
    "    recall = np.zeros(len(Ytokenizer.tokens))\n",
    "    precision = np.zeros(len(Ytokenizer.tokens))\n",
    "    for i in range(len(Ytokenizer.tokens)):\n",
    "        recall[i] = confusionMatrix[i][i] / np.sum(confusionMatrix[i])\n",
    "        precision[i] = confusionMatrix[i][i] / np.sum(confusionMatrix[:, i])\n",
    "        \n",
    "    for i in range(len(Ytokenizer.tokens)):\n",
    "        print('Recall (' +  list(Ytokenizer.txt2token.keys())[i] + ') :', round(recall[i], 3))\n",
    "        print('Precision (' +  list(Ytokenizer.txt2token.keys())[i] + ') :', round(precision[i], 3))\n",
    "        print()\n",
    "        \n",
    "printConfusionMatrix(confusMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/40], Loss: 1.3789770603179932\n",
      "Epoch [2/30], Step [1/40], Loss: 0.4532298445701599\n",
      "Epoch [3/30], Step [1/40], Loss: 0.15091554820537567\n",
      "Epoch [4/30], Step [1/40], Loss: 0.11187899112701416\n",
      "Epoch [5/30], Step [1/40], Loss: 0.005505539011210203\n",
      "Epoch [6/30], Step [1/40], Loss: 0.06953097879886627\n",
      "Epoch [7/30], Step [1/40], Loss: 0.05879200994968414\n",
      "Epoch [8/30], Step [1/40], Loss: 0.06436994671821594\n",
      "Epoch [9/30], Step [1/40], Loss: 0.007900312542915344\n",
      "Epoch [10/30], Step [1/40], Loss: 0.008890537545084953\n",
      "Epoch [11/30], Step [1/40], Loss: 0.0021895801182836294\n",
      "Epoch [12/30], Step [1/40], Loss: 0.0002807286800816655\n",
      "Epoch [13/30], Step [1/40], Loss: 0.007359320763498545\n",
      "Epoch [14/30], Step [1/40], Loss: 0.0011824186658486724\n",
      "Epoch [15/30], Step [1/40], Loss: 0.0014012185856699944\n",
      "Epoch [16/30], Step [1/40], Loss: 0.04276882857084274\n",
      "Epoch [17/30], Step [1/40], Loss: 0.17601022124290466\n",
      "Epoch [18/30], Step [1/40], Loss: 0.9982756972312927\n",
      "Epoch [19/30], Step [1/40], Loss: 0.001604574266821146\n",
      "Epoch [20/30], Step [1/40], Loss: 0.002529545919969678\n",
      "Epoch [21/30], Step [1/40], Loss: 0.0005187028436921537\n",
      "Epoch [22/30], Step [1/40], Loss: 0.0006702243117615581\n",
      "Epoch [23/30], Step [1/40], Loss: 0.08489932119846344\n",
      "Epoch [24/30], Step [1/40], Loss: 0.0401834100484848\n",
      "Epoch [25/30], Step [1/40], Loss: 0.000981681514531374\n",
      "Epoch [26/30], Step [1/40], Loss: 0.0344463512301445\n",
      "Epoch [27/30], Step [1/40], Loss: 0.001882112817838788\n",
      "Epoch [28/30], Step [1/40], Loss: 0.04663879796862602\n",
      "Epoch [29/30], Step [1/40], Loss: 0.00040371360955759883\n",
      "Epoch [30/30], Step [1/40], Loss: 0.0004803044721484184\n",
      "Finished Training\n",
      "Accuracy: 53.75816993464052%\n",
      "Confusion matrix:\n",
      "\t\tfalse (real)\tmixture (real)\tother (real)\ttrue (real)\n",
      "false (pred)\t273\t\t32\t\t22\t\t110\n",
      "mixture (pred)\t27\t\t17\t\t8\t\t59\n",
      "other (pred)\t3\t\t1\t\t0\t\t2\n",
      "true (pred)\t12\t\t6\t\t1\t\t39\n",
      "\n",
      "Recall (false) : 0.625\n",
      "Precision (false) : 0.867\n",
      "\n",
      "Recall (mixture) : 0.153\n",
      "Precision (mixture) : 0.304\n",
      "\n",
      "Recall (other) : 0.0\n",
      "Precision (other) : 0.0\n",
      "\n",
      "Recall (true) : 0.672\n",
      "Precision (true) : 0.186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def train_nn(X, y, model, criterion, optimizer, num_epochs=5):\n",
    "    train_dataset = TextDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts = texts.float()\n",
    "            labels = labels.long()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "def test_nn(X: torch.Tensor, y: torch.Tensor, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        confusMatrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "        for texts, labels in zip(X, y):\n",
    "            texts = texts.float().unsqueeze(0)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicted = int(predicted.item())\n",
    "            labels = int(labels.item())\n",
    "            \n",
    "            confusMatrix[predicted][labels] += 1\n",
    "\n",
    "        print(f'Accuracy: {100 * correct / total}%')\n",
    "        printConfusionMatrix(confusMatrix)\n",
    "        \n",
    "        \n",
    "# Hyperparameters\n",
    "input_size = len(Xtokenizer.tokens)\n",
    "hidden_size = 1000\n",
    "num_classes = len(Ytokenizer.tokens)\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data\n",
    "Xtrain = torch.tensor(XhistTrain).float()\n",
    "Xtest = torch.tensor(XhistTest).float()\n",
    "YtokenTrain1D = torch.tensor([y[0] for y in YtokenTrain]).long()\n",
    "YtokenTest1D = torch.tensor([y[0] for y in YtokenTest]).long()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    Xtrain = Xtrain.to(device)\n",
    "    Xtest = Xtest.to(device)\n",
    "    YtokenTrain1D = YtokenTrain1D.to(device)\n",
    "    YtokenTest1D = YtokenTest1D.to(device)\n",
    "    \n",
    "train_nn(Xtrain, YtokenTrain1D, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Test the model\n",
    "test_nn(Xtest, YtokenTest1D, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
