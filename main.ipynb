{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Union, Dict, Generic, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>our rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a228e0e</td>\n",
       "      <td>Distracted driving causes more deaths in Canad...</td>\n",
       "      <td>You Can Be Fined $1,500 If Your Passenger Is U...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c605a1</td>\n",
       "      <td>Missouri politicians have made statements afte...</td>\n",
       "      <td>Missouri lawmakers condemn Las Vegas shooting</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c3dea290</td>\n",
       "      <td>Home Alone 2: Lost in New York is full of viol...</td>\n",
       "      <td>CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...</td>\n",
       "      <td>mixture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14e8eb6</td>\n",
       "      <td>But things took a turn for the worse when riot...</td>\n",
       "      <td>Obama’s Daughters Caught on Camera Burning US ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faf024d6</td>\n",
       "      <td>It’s no secret that Epstein and Schiff share a...</td>\n",
       "      <td>Leaked Visitor Logs Reveal Schiff’s 78 Visits ...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>47423bb6</td>\n",
       "      <td>More than four million calls to the taxman are...</td>\n",
       "      <td>Taxman fails to answer four million calls a ye...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>097c142a</td>\n",
       "      <td>More under-18s are being taken to court for se...</td>\n",
       "      <td>Police catch 11‑year‑olds being used to sell d...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>08bc59f4</td>\n",
       "      <td>The Government’s much vaunted Help to Buy Isa ...</td>\n",
       "      <td>Help to Buy Isa scandal: 500,000 first-time bu...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>af3393ce</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>a39d07df</td>\n",
       "      <td>The late Robin Williams once called cocaine “G...</td>\n",
       "      <td>A coke-snorting generation of hypocrites</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1264 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     public_id                                               text  \\\n",
       "0     5a228e0e  Distracted driving causes more deaths in Canad...   \n",
       "1     30c605a1  Missouri politicians have made statements afte...   \n",
       "2     c3dea290  Home Alone 2: Lost in New York is full of viol...   \n",
       "3     f14e8eb6  But things took a turn for the worse when riot...   \n",
       "4     faf024d6  It’s no secret that Epstein and Schiff share a...   \n",
       "...        ...                                                ...   \n",
       "1259  47423bb6  More than four million calls to the taxman are...   \n",
       "1260  097c142a  More under-18s are being taken to court for se...   \n",
       "1261  08bc59f4  The Government’s much vaunted Help to Buy Isa ...   \n",
       "1262  af3393ce  The late Robin Williams once called cocaine “G...   \n",
       "1263  a39d07df  The late Robin Williams once called cocaine “G...   \n",
       "\n",
       "                                                  title our rating  \n",
       "0     You Can Be Fined $1,500 If Your Passenger Is U...      false  \n",
       "1         Missouri lawmakers condemn Las Vegas shooting    mixture  \n",
       "2     CBC Cuts Donald Trump's 'Home Alone 2' Cameo O...    mixture  \n",
       "3     Obama’s Daughters Caught on Camera Burning US ...      false  \n",
       "4     Leaked Visitor Logs Reveal Schiff’s 78 Visits ...      false  \n",
       "...                                                 ...        ...  \n",
       "1259  Taxman fails to answer four million calls a ye...       true  \n",
       "1260  Police catch 11‑year‑olds being used to sell d...       true  \n",
       "1261  Help to Buy Isa scandal: 500,000 first-time bu...      false  \n",
       "1262           A coke-snorting generation of hypocrites       true  \n",
       "1263           A coke-snorting generation of hypocrites       true  \n",
       "\n",
       "[1264 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain = pd.read_csv('data/HAI817_Projet_train.csv')\n",
    "dataTest = pd.read_csv('data/HAI817_Projet_test.csv')\n",
    "dataTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Luna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk download\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distracted driving causes deaths canada impaired driving every province territory laws driving operating cell phone tell passengers stay phones driving measures necessary distracted driving claimed lives impaired driving provinces like british columbia ontario quebec alberta nova scotia manitoba newfoundland labrador mobile phones even held passenger dangerous distraction driver starting next week distracted screen held passenger attracts penalty three demerit points drivers screens mix matter holding device using facetime taking selfies driver showing driver funny cat video provinces mobile phone categorised visual display unit meaning considered akin television screen important practice safe driving sake fellow drivers canada cracking distracted driving problem rollout stricter laws impose harsher penalties heftier fines guilty offenders taking effect next week adds serious penalties convicted distracted driving'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # remove punctuation\n",
    "    words = [word for word in words if word not in punctuations]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "XpreprocessTrain = dataTrain['text'].apply(remove_stop_words)\n",
    "XpreprocessTest = dataTest['text'].apply(remove_stop_words)\n",
    "ytxtTrain = dataTrain['our rating']\n",
    "ytxtTest = dataTest['our rating']\n",
    "XpreprocessTrain[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['mixture', 'other'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m XhistTest \u001b[38;5;241m=\u001b[39m Xtokenizer\u001b[38;5;241m.\u001b[39mhistogram(XpreprocessTest)\n\u001b[0;32m     41\u001b[0m Ytokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mytxtTrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmixture\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mother\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m ytxtTest\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixture\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     44\u001b[0m Ytokenizer\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39marray(ytxtTrain), np\u001b[38;5;241m.\u001b[39marray(ytxtTest))))\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\series.py:5336\u001b[0m, in \u001b[0;36mSeries.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5241\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5248\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5251\u001b[0m \u001b[38;5;124;03m    Return Series with specified index labels removed.\u001b[39;00m\n\u001b[0;32m   5252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5334\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   5335\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5338\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5342\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5343\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['mixture', 'other'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "class Tokenizer:\n",
    "    def __init__(self, dtype: type = np.int64):\n",
    "        self.txt2token = {}\n",
    "        self.token2txt = {}\n",
    "        self.tokens = set()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, text: np.ndarray):\n",
    "        count = 0\n",
    "        for txt in text:\n",
    "            for word in txt.split():\n",
    "                if word not in self.txt2token:\n",
    "                    self.txt2token[word] = count\n",
    "                    self.token2txt[count] = word\n",
    "                    count += 1\n",
    "                self.tokens.add(self.txt2token[word])\n",
    "\n",
    "    def histogram(self, text):\n",
    "        tokenized_text = np.zeros((len(text), len(self.tokens)), dtype=self.dtype)\n",
    "        for i, txt in enumerate(text):\n",
    "            for word in txt.split():\n",
    "                tokenized_text[i][self.txt2token[word]] += 1\n",
    "\n",
    "        return tokenized_text\n",
    "    \n",
    "    def transform(self, text) -> list[np.ndarray]:\n",
    "        tokenized_text = []\n",
    "        for txt in text:\n",
    "            tokenized_text.append(np.array([self.txt2token[word] for word in txt.split() if word in self.txt2token], dtype=self.dtype))\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "Xtokenizer = Tokenizer(dtype=np.int64)\n",
    "Xtokenizer.fit(np.concatenate((np.array(XpreprocessTrain), np.array(XpreprocessTest))))\n",
    "XtokenTrain = Xtokenizer.transform(XpreprocessTrain)\n",
    "XhistTrain = Xtokenizer.histogram(XpreprocessTrain)\n",
    "XtokenTest = Xtokenizer.transform(XpreprocessTest)\n",
    "XhistTest = Xtokenizer.histogram(XpreprocessTest)\n",
    "\n",
    "Ytokenizer = Tokenizer(dtype=np.uint8)\n",
    "# ytxtTrain.drop(\"mixture\", inplace=True)\n",
    "# ytxtTest.drop(['mixture', 'other'])\n",
    "Ytokenizer.fit(np.concatenate((np.array(ytxtTrain), np.array(ytxtTest))))\n",
    "YtokenTrain = Ytokenizer.transform(ytxtTrain)\n",
    "YtokenTest = Ytokenizer.transform(ytxtTest)\n",
    "\n",
    "XtokenTrain[0], YtokenTrain[0], Ytokenizer.txt2token.keys()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling and undersampling\n",
    "def balance_data(X, Xhist, Y):\n",
    "    # Convert X and Y to numpy arrays for easier manipulation\n",
    "    Y_np = np.array([y[0] for y in Y])\n",
    "    \n",
    "    unique_classes, counts = np.unique(Y_np, return_counts=True)\n",
    "    max_count = max(counts)\n",
    "    \n",
    "    X_balanced = []\n",
    "    Xhist_balanced = []\n",
    "    Y_balanced = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(Y_np == cls)[0]\n",
    "        cls_samples = [X[i] for i in cls_indices]\n",
    "        cls_hist_samples = Xhist[cls_indices]\n",
    "        cls_labels = [Y[i] for i in cls_indices]\n",
    "        \n",
    "        if counts[cls] < max_count:\n",
    "            # Oversampling\n",
    "            num_to_add = max_count - counts[cls]\n",
    "            additional_indices = np.random.choice(cls_indices, num_to_add, replace=True)\n",
    "            additional_samples = [X[i] for i in additional_indices]\n",
    "            additional_hist_samples = Xhist[additional_indices]\n",
    "            additional_labels = [Y[i] for i in additional_indices]\n",
    "            \n",
    "            X_balanced.extend(cls_samples + additional_samples)\n",
    "            Xhist_balanced.extend(np.vstack((cls_hist_samples, additional_hist_samples)))\n",
    "            Y_balanced.extend(cls_labels + additional_labels)\n",
    "        else:\n",
    "            # Undersampling\n",
    "            indices_to_keep = np.random.choice(cls_indices, max_count, replace=False)\n",
    "            X_balanced.extend([X[i] for i in indices_to_keep])\n",
    "            Xhist_balanced.extend(Xhist[indices_to_keep])\n",
    "            Y_balanced.extend([Y[i] for i in indices_to_keep])\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    perm = np.random.permutation(len(Y_balanced))\n",
    "    X_balanced = [X_balanced[i] for i in perm]\n",
    "    Xhist_balanced = np.array(Xhist_balanced)[perm]\n",
    "    Y_balanced = [Y_balanced[i] for i in perm]\n",
    "    \n",
    "    return X_balanced, Xhist_balanced, Y_balanced\n",
    "\n",
    "Xtrain_balanced, Xhist_train_balanced, Ytrain_balanced = balance_data(XtokenTrain, XhistTrain, YtokenTrain)\n",
    "Xtest_balanced, Xhist_test_balanced, Ytest_balanced = balance_data(XtokenTest, XhistTest, YtokenTest)\n",
    "len(Xtrain_balanced), len(Xtest_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3], dtype=uint8), array([578, 578, 578, 578], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_classes, counts = np.unique(np.array([y[0] for y in Ytrain_balanced]), return_counts=True)\n",
    "unique_classes, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.32777778]\n"
     ]
    }
   ],
   "source": [
    "# naive bayes\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_prior = {}\n",
    "        self.word_likelihood = {}\n",
    "        self.vocab = set()\n",
    "        self.word_count = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(np.concatenate(y))\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        self.word_count = {c: Counter() for c in self.classes}\n",
    "        class_count = Counter()\n",
    "\n",
    "        for i in range(total_samples):\n",
    "            label = y[i][0]\n",
    "            class_count[label] += 1\n",
    "            for word in X[i]:\n",
    "                self.word_count[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "        self.class_prior = {c: count / total_samples for c, count in class_count.items()}\n",
    "        \n",
    "        self.word_likelihood = {c: {} for c in self.classes}\n",
    "        vocab_size = len(self.vocab)\n",
    "        for c in self.classes:\n",
    "            total_words = sum(self.word_count[c].values())\n",
    "            for word in self.vocab:\n",
    "                self.word_likelihood[c][word] = (self.word_count[c][word] + 1) / (total_words + vocab_size)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            log_probs = {}\n",
    "            for c in self.classes:\n",
    "                log_probs[c] = np.log(self.class_prior[c])\n",
    "                for word in sample:\n",
    "                    if word in self.vocab:\n",
    "                        log_probs[c] += np.log(self.word_likelihood[c][word])\n",
    "                    else:\n",
    "                        log_probs[c] += np.log(1 / (sum(self.word_count[c].values()) + len(self.vocab)))\n",
    "            \n",
    "            predicted_class = max(log_probs, key=log_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "# Train the model\n",
    "model = NaiveBayesClassifier()\n",
    "model.fit(Xtrain_balanced, Ytrain_balanced)\n",
    "\n",
    "# Predict the test set\n",
    "Ypred = model.predict(Xtest_balanced)\n",
    "\n",
    "confusMatrix = np.zeros((len(Ytokenizer.tokens), len(Ytokenizer.tokens)), dtype=np.int64)\n",
    "\n",
    "accuracySum = 0\n",
    "for i in range(len(Ypred)):\n",
    "    accuracySum += Ypred[i] == Ytest_balanced[i]\n",
    "    confusMatrix[Ytest_balanced[i][0]][Ypred[i]] += 1\n",
    "accuracy = accuracySum / len(Ypred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "\t\tfalse (real)\tmixture (real)\tother (real)\ttrue (real)\n",
      "false (pred)\t285\t\t13\t\t7\t\t10\n",
      "mixture (pred)\t199\t\t77\t\t13\t\t26\n",
      "other (pred)\t252\t\t54\t\t0\t\t9\n",
      "true (pred)\t218\t\t42\t\t4\t\t51\n",
      "\n",
      "Recall (false) : 0.905\n",
      "Precision (false) : 0.299\n",
      "\n",
      "Recall (mixture) : 0.244\n",
      "Precision (mixture) : 0.414\n",
      "\n",
      "Recall (other) : 0.0\n",
      "Precision (other) : 0.0\n",
      "\n",
      "Recall (true) : 0.162\n",
      "Precision (true) : 0.531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printConfusionMatrix(confusionMatrix):\n",
    "    print('Confusion matrix:')\n",
    "    # format nicely with column and row headers\n",
    "    print('\\t' + ''.join([\"\\t\" + s + \" (real)\" for s in Ytokenizer.txt2token.keys()]))\n",
    "    for i in range(len(Ytokenizer.txt2token.keys())):\n",
    "        print(list(Ytokenizer.txt2token.keys())[i] + ' (pred)', end='\\t')\n",
    "        print('\\t\\t'.join(str(confusionMatrix[i][j]) for j in range(len(Ytokenizer.tokens))))\n",
    "        \n",
    "    print()\n",
    "\n",
    "    # compute recall and precision\n",
    "    recall = np.zeros(len(Ytokenizer.tokens))\n",
    "    precision = np.zeros(len(Ytokenizer.tokens))\n",
    "    for i in range(len(Ytokenizer.tokens)):\n",
    "        recall[i] = confusionMatrix[i][i] / np.sum(confusionMatrix[i])\n",
    "        precision[i] = confusionMatrix[i][i] / np.sum(confusionMatrix[:, i])\n",
    "        \n",
    "    for i in range(len(Ytokenizer.tokens)):\n",
    "        print('Recall (' +  list(Ytokenizer.txt2token.keys())[i] + ') :', round(recall[i], 3))\n",
    "        print('Precision (' +  list(Ytokenizer.txt2token.keys())[i] + ') :', round(precision[i], 3))\n",
    "        print()\n",
    "        \n",
    "printConfusionMatrix(confusMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/73], Loss: 1.4006608724594116\n",
      "Epoch [2/30], Step [1/73], Loss: 0.10689215362071991\n",
      "Epoch [3/30], Step [1/73], Loss: 0.010510491207242012\n",
      "Epoch [4/30], Step [1/73], Loss: 0.0036225696094334126\n",
      "Epoch [5/30], Step [1/73], Loss: 0.0004917188198305666\n",
      "Epoch [6/30], Step [1/73], Loss: 0.010139763355255127\n",
      "Epoch [7/30], Step [1/73], Loss: 0.0029680593870580196\n",
      "Epoch [8/30], Step [1/73], Loss: 0.0037949339020997286\n",
      "Epoch [9/30], Step [1/73], Loss: 0.007726357318460941\n",
      "Epoch [10/30], Step [1/73], Loss: 0.05107056722044945\n",
      "Epoch [11/30], Step [1/73], Loss: 0.08436957001686096\n",
      "Epoch [12/30], Step [1/73], Loss: 0.00899070966988802\n",
      "Epoch [13/30], Step [1/73], Loss: 0.0001836079463828355\n",
      "Epoch [14/30], Step [1/73], Loss: 0.00010499460768187419\n",
      "Epoch [15/30], Step [1/73], Loss: 0.018175577744841576\n",
      "Epoch [16/30], Step [1/73], Loss: 0.0002201511524617672\n",
      "Epoch [17/30], Step [1/73], Loss: 0.007911665365099907\n",
      "Epoch [18/30], Step [1/73], Loss: 0.027058914303779602\n",
      "Epoch [19/30], Step [1/73], Loss: 0.007401290815323591\n",
      "Epoch [20/30], Step [1/73], Loss: 2.0126004528719932e-05\n",
      "Epoch [21/30], Step [1/73], Loss: 0.016361460089683533\n",
      "Epoch [22/30], Step [1/73], Loss: 5.7356162869837135e-05\n",
      "Epoch [23/30], Step [1/73], Loss: 6.822776776971295e-05\n",
      "Epoch [24/30], Step [1/73], Loss: 0.014836885966360569\n",
      "Epoch [25/30], Step [1/73], Loss: 0.03427446633577347\n",
      "Epoch [26/30], Step [1/73], Loss: 0.004893163219094276\n",
      "Epoch [27/30], Step [1/73], Loss: 0.2033831924200058\n",
      "Epoch [28/30], Step [1/73], Loss: 0.00010423421917948872\n",
      "Epoch [29/30], Step [1/73], Loss: 0.010275382548570633\n",
      "Epoch [30/30], Step [1/73], Loss: 0.09459925442934036\n",
      "Finished Training\n",
      "Accuracy: 32.06349206349206%\n",
      "Confusion matrix:\n",
      "\t\tfalse (real)\tmixture (real)\tother (real)\ttrue (real)\n",
      "false (pred)\t286\t\t229\t\t269\t\t207\n",
      "mixture (pred)\t23\t\t65\t\t30\t\t68\n",
      "other (pred)\t1\t\t0\t\t16\t\t3\n",
      "true (pred)\t5\t\t21\t\t0\t\t37\n",
      "\n",
      "Recall (false) : 0.289\n",
      "Precision (false) : 0.908\n",
      "\n",
      "Recall (mixture) : 0.349\n",
      "Precision (mixture) : 0.206\n",
      "\n",
      "Recall (other) : 0.8\n",
      "Precision (other) : 0.051\n",
      "\n",
      "Recall (true) : 0.587\n",
      "Precision (true) : 0.117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "def train_nn(X, y, model, criterion, optimizer, num_epochs=5):\n",
    "    train_dataset = TextDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (texts, labels) in enumerate(train_loader):\n",
    "            texts = texts.float()\n",
    "            labels = labels.long()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}')\n",
    "                \n",
    "    print('Finished Training')\n",
    "    \n",
    "def test_nn(X: torch.Tensor, y: torch.Tensor, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        confusMatrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "        for texts, labels in zip(X, y):\n",
    "            texts = texts.float().unsqueeze(0)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            predicted = int(predicted.item())\n",
    "            labels = int(labels.item())\n",
    "            \n",
    "            confusMatrix[predicted][labels] += 1\n",
    "\n",
    "        print(f'Accuracy: {100 * correct / total}%')\n",
    "        printConfusionMatrix(confusMatrix)\n",
    "        \n",
    "        \n",
    "# Hyperparameters\n",
    "input_size = len(Xtokenizer.tokens)\n",
    "hidden_size = 1000\n",
    "num_classes = len(Ytokenizer.tokens)\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare data\n",
    "Xtrain = torch.tensor(Xhist_train_balanced).float()\n",
    "Xtest = torch.tensor(Xhist_test_balanced).float()\n",
    "YtokenTrain1D = torch.tensor([y[0] for y in Ytrain_balanced]).long()\n",
    "YtokenTest1D = torch.tensor([y[0] for y in Ytest_balanced]).long()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    Xtrain = Xtrain.to(device)\n",
    "    Xtest = Xtest.to(device)\n",
    "    YtokenTrain1D = YtokenTrain1D.to(device)\n",
    "    YtokenTest1D = YtokenTest1D.to(device)\n",
    "    \n",
    "train_nn(Xtrain, YtokenTrain1D, model, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Test the model\n",
    "test_nn(Xtest, YtokenTest1D, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
